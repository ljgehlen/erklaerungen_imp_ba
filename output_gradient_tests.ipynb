{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe3c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import get_agnews_prepraped\n",
    "from models import SentenceCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af529358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for vocab in ./datasets/AG_NEWS/vocab.torch\n",
      "Load Prepared Data\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "size_train_batch = 64\n",
    "size_test_batch = 1028\n",
    "\n",
    "train_set, test_set, vocab_size, n_classes, vocab = get_agnews_prepraped(random_state=42,\n",
    "                                                                   batch_sizes=(size_train_batch, size_test_batch))\n",
    "X_test, Y_test = next(iter(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf32341",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "model = SentenceCNN(n_classes=n_classes, embed_dim=embed_dim, vocab_size=vocab_size)\n",
    "model.load_state_dict(torch.load('./results/001/model_1'))\n",
    "loss_fun = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8924b859",
   "metadata": {},
   "source": [
    "Code bei Sebastian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d71a2bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_outputs(inference_fn, data, model, device, batch_size=256):\n",
    "\n",
    "    _data = DataLoader(TensorDataset(data), shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    try:\n",
    "        _y_out = []\n",
    "        for x in _data:\n",
    "            _y = inference_fn(x[0].to(device))\n",
    "            _y_out.append(_y.cpu())\n",
    "        return torch.vstack(_y_out)\n",
    "    except RuntimeError as re:\n",
    "        if \"CUDA out of memory\" in str(re):\n",
    "            model.to('cpu')\n",
    "            outputs = _get_outputs(inference_fn, data, model, 'cpu')\n",
    "            model.to('cuda')\n",
    "            return outputs\n",
    "        else:\n",
    "            raise re\n",
    "\n",
    "\n",
    "def _get_predictions(inference_fn, data, model, device):\n",
    "    return torch.argmax(_get_outputs(inference_fn, data, model, device), dim=1)\n",
    "\n",
    "\n",
    "def validate(inference_fn, model, X, Y):\n",
    "\n",
    "    if inference_fn is None:\n",
    "        inference_fn = model\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    _y_pred = _get_predictions(inference_fn, X, model, device)\n",
    "    model.train()\n",
    "\n",
    "    acc = torch.mean((Y == _y_pred).to(torch.float)).detach().cpu().item()  # mean expects float, not bool (or int)\n",
    "    return acc\n",
    "\n",
    "\n",
    "# Aufrufsequence:\n",
    "#\n",
    "# acc_val.append(validate(inference_fn, model, *te_data))\n",
    "#\n",
    "# def train(model, optim, loss_fn, tr_data: DataLoader, te_data: tuple, inference_fn=None, \\\n",
    "#              n_batches_max=10, device='cuda'):\n",
    "#\n",
    "# active_model, last_acc = train(model, optimizer, loss_fun, tr_data, te_data,\n",
    "#              inference_fn=model.forward_softmax, device=device, n_batches_max=n_batches)\n",
    "#\n",
    "# def imp_loop(path, model, init_model, best_model, device, optimizer, loss_fun, n_batches, n_epochs,\n",
    "#            pruning_type, pruning_perc, n_iterations, tr_data: DataLoader, te_data: tuple):\n",
    "#\n",
    "# imp_loop(path, model, init_model, best_model, device, optimizer, loss_fun, n_batches, n_epochs,\n",
    "#            pruning_type, pruning_perc, n_iterations, train_set, (X_test, Y_test))\n",
    "#\n",
    "# X_test, Y_test = next(iter(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c62aea8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8793774247169495\n"
     ]
    }
   ],
   "source": [
    "# Ausgabe des Modells auf Sebastian Code mit test_set:\n",
    "\n",
    "inference_fn = model.forward_softmax\n",
    "acc_exp01 = validate(inference_fn, model, X_test, Y_test)\n",
    "\n",
    "print(acc_exp01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf2c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untersuchungen mit angepassten Batch_Sizes in _get_outputs:\n",
    "\n",
    "def _get_predictions_with_batch_s(inference_fn, data, model, device, batch_size):\n",
    "    return torch.argmax(_get_outputs(inference_fn, data, model, device, batch_size), dim=1)\n",
    "\n",
    "\n",
    "def validate_with_batch_s(inference_fn, model, X, Y, batch_size):\n",
    "\n",
    "    if inference_fn is None:\n",
    "        inference_fn = model\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    _y_pred = _get_predictions_with_batch_s(inference_fn, X, model, device, batch_size)\n",
    "    model.train()\n",
    "\n",
    "    acc = torch.mean((Y == _y_pred).to(torch.float)).detach().cpu().item()  # mean expects float, not bool (or int)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e182a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size = 1:\n",
      "0.8793774247169495\n",
      "batch size = 1028:\n",
      "0.8793774247169495\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "acc_exp02 = validate_with_batch_s(inference_fn, model, X_test, Y_test, batch_size)\n",
    "\n",
    "print(f'batch size = {batch_size}:')\n",
    "print(acc_exp02)\n",
    "\n",
    "batch_size = 1028\n",
    "acc_exp03 = validate_with_batch_s(inference_fn, model, X_test, Y_test, batch_size)\n",
    "\n",
    "print(f'batch size = {batch_size}:')\n",
    "print(acc_exp03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9022af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anpassung mit inference_fn = forward_embedded_softmax\n",
    "def _get_outputs_from_emb(inference_fn, data, model, device, batch_size=256):\n",
    "\n",
    "    _data = DataLoader(TensorDataset(data), shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    try:\n",
    "        _y_out = []\n",
    "        for x in _data:\n",
    "            _y = inference_fn(model.embed_input(x[0]).to(device))\n",
    "            _y_out.append(_y.cpu())\n",
    "        return torch.vstack(_y_out)\n",
    "    except RuntimeError as re:\n",
    "        if \"CUDA out of memory\" in str(re):\n",
    "            model.to('cpu')\n",
    "            outputs = _get_outputs(inference_fn, data, model, 'cpu')\n",
    "            model.to('cuda')\n",
    "            return outputs\n",
    "        else:\n",
    "            raise re\n",
    "\n",
    "\n",
    "def _get_predictions_from_emb(inference_fn, data, model, device, batch_size=256):\n",
    "    return torch.argmax(_get_outputs_from_emb(inference_fn, data, model, device, batch_size), dim=1)\n",
    "\n",
    "\n",
    "def validate_from_emb(inference_fn, model, X, Y, batch_size=256):\n",
    "\n",
    "    if inference_fn is None:\n",
    "        inference_fn = model\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    _y_pred = _get_predictions_from_emb(inference_fn, X, model, device, batch_size)\n",
    "    model.train()\n",
    "\n",
    "    acc = torch.mean((Y == _y_pred).to(torch.float)).detach().cpu().item()  # mean expects float, not bool (or int)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b9e549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size = Standart 256:\n",
      "0.8793774247169495\n",
      "batch size = 1:\n",
      "0.8793774247169495\n",
      "batch size = 1028:\n",
      "0.8793774247169495\n"
     ]
    }
   ],
   "source": [
    "inference_fn = model.forward_embedded_softmax\n",
    "\n",
    "print(f'batch size = Standart 256:')\n",
    "acc_exp04 = validate_from_emb(inference_fn, model, X_test, Y_test)\n",
    "print(acc_exp04)\n",
    "\n",
    "batch_size = 1\n",
    "acc_exp05 = validate_from_emb(inference_fn, model, X_test, Y_test, batch_size)\n",
    "\n",
    "print(f'batch size = {batch_size}:')\n",
    "print(acc_exp05)\n",
    "\n",
    "batch_size = 1028\n",
    "acc_exp06 = validate_from_emb(inference_fn, model, X_test, Y_test, batch_size)\n",
    "\n",
    "print(f'batch size = {batch_size}:')\n",
    "print(acc_exp06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ea38cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_outputs_emb_simple(inference_fn, data, model, device, batch_size=1):\n",
    "\n",
    "    _data = DataLoader(TensorDataset(data), shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    _y_out = []\n",
    "    for x in _data:\n",
    "        _y = inference_fn(model.embed_input(x[0]).to(device))\n",
    "        _y_out.append(_y.cpu())\n",
    "    return torch.vstack(_y_out)\n",
    "\n",
    "def _get_predictions_emb_simple(inference_fn, data, model, device):\n",
    "    return torch.argmax(_get_outputs_emb_simple(inference_fn, data, model, device), dim=1)\n",
    "\n",
    "\n",
    "def validate_emb_simple(inference_fn, model, X, Y):\n",
    "\n",
    "    if inference_fn is None:\n",
    "        inference_fn = model\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    _y_pred = _get_predictions_emb_simple(inference_fn, X, model, device)\n",
    "    model.train()\n",
    "\n",
    "    acc = torch.mean((Y == _y_pred).to(torch.float)).detach().cpu().item()  # mean expects float, not bool (or int)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ea55802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8793774247169495\n"
     ]
    }
   ],
   "source": [
    "acc_exp07 = validate_emb_simple(inference_fn, model, X_test, Y_test)\n",
    "\n",
    "print(acc_exp07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "263ff359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_outputs_and_gradients(inference_fn, X, Y, model, loss_fun, device, batch_size=1):\n",
    "\n",
    "    _data = DataLoader(TensorDataset(X), shuffle=False, batch_size=batch_size)\n",
    "    _labels = DataLoader(TensorDataset(Y), shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    _y_out = []\n",
    "    _grads = []\n",
    "    for text, label in zip(_data, _labels):\n",
    "        emb = model.embed_input(text[0]).to(device)\n",
    "        emb.requires_grad = True\n",
    "        _y = inference_fn(emb)\n",
    "        _y_out.append(_y.cpu())\n",
    "        _loss = loss_fun(_y, label[0])\n",
    "        grad = torch.autograd.grad(_loss, emb, retain_graph=True)[0].data\n",
    "        _grads.append(grad)\n",
    "    return torch.vstack(_y_out), torch.vstack(_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56dbe9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8793774247169495\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "_y_outs, _grads = _get_outputs_and_gradients(inference_fn, X_test, Y_test, model, loss_fun, device)\n",
    "_y_pred = torch.argmax(_y_outs, dim=1)\n",
    "model.train()\n",
    "print(torch.mean((Y_test == _y_pred).to(torch.float)).detach().cpu().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "500c36bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8793774319066148\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(_y_pred)):\n",
    "    if _y_pred[i] == Y_test[i]:\n",
    "        correct += 1\n",
    "print(correct/len(_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa5e7aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8793774247169495\n"
     ]
    }
   ],
   "source": [
    "from rules import _get_gradients_and_outputs\n",
    "\n",
    "_y_outs, _grads, _y_pred, acc = _get_gradients_and_outputs(inference_fn, X_test, Y_test, model, loss_fun, device)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f03ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
